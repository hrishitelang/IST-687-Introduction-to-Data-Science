---
output:
  pdf_document: default
  html_document: default
---
# Intro to Data Science - Lab 11

##### Copyright 2021, Jeffrey Stanton, Jeffrey Saltz, and Jasmina Tacheva


```{r}
# Enter your name here: Hrishikesh Telang
```

### Attribution statement: (choose only one and delete the rest)


```{r}
# 1. I did this homework by myself, with help from the book and the professor.
```

#Instructions: Text mining plays an important role in many industries because of the prevalence of text in the interactions between customers and company representatives. Even when the customer interaction is by speech, rather than by chat or email, speech to text algorithms have gotten so good that transcriptions of these spoken word interactions are often available. To an increasing extent, a data scientist needs to be able to wield tools that turn a body of text into actionable insights. 

#In this exercise, we work with a small number of social media posts on the topic of climate change. Make sure to install and library the readr and quanteda package before starting the exercise. Please include an attribution statement (see syllabus).

#1.	Read in the following data set with read_csv(): https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv


#The name of the file is “ClimatePosts.csv”. Store the data in a data frame called tweetDF. Use str(tweetDF) to summarize the data. Add a comment describing what you see. Make sure to explain what each of the three variables contains.
```{r}
library(readr)
tweetDF <- read_csv('https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv')
#'tweetDF' basically contains three variables such as: 1) tweet_id which confirms 
#the title of the tweet, how skeptic the tweet in the form of binary numbers 
#and the tweet written
head(tweetDF)
tail(tweetDF)
#str(tweetDF)
```


#2.	Use the corpus commands to turn the text variable into a quanteda corpus. You can use the IDs as the document titles with the following command:
```{r}
#install.packages('quanteda')
library(quanteda)
tweetCorpus <- corpus(tweetDF$Tweet, docnames=tweetDF$ID)
#this command creates separate documents as key-value pair 
tweetCorpus
```

#3.	Next, convert the corpus into a document-feature matrix (DFM).  Before you do that you can use “tokens” to remove punctuation and stop words. Use this code:
```{r}
toks <- tokens(tweetCorpus, remove_punct=TRUE)
#This code removes punctuation.
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")
#This code removes stop words.
```

#Here’s a command that will create the DFM:
```{r}
tweetDFM <- dfm(toks_nostop, tolower = TRUE)
```

#4.	Type tweetDFM at the console to find out the basic characteristic of the DFM (the number of terms, the number of documents, and the sparsity of the matrix). Write a comment describing what you observe.
```{r}
tweetDFM
#tweetDFM consists of 18 documents, 223 features and the matrix
#is 93.20% sparse with 0 docvars.
```


#5.	Create a wordcloud from the DFM using the following command. Write a comment describing notable features of the wordcloud:
```{r}
#install.packages('quanteda.textplots')
library(quanteda.textplots)
textplot_wordcloud(tweetDFM, min_count = 1)
#This describes the frequency of the words in the tweets
```

#6.	Using textstat_frequency() from the quanteda.textstats package, show the 10 most frequent words, and how many times each was used/mentioned.
```{r}
#install.packages('quanteda.textstats')
library(quanteda.textstats)
textstat_frequency(tweetDFM)
#This code shows the words with the most frequency ranked in an ascending order
```


#7.	Next, we will read in dictionaries of positive and negative words to see what we can match up to the text in our DFM. Here’s a line of code for reading in the list of positive words:
```{r}
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]
```

#Create a similar line of code to read in the negative words, with the following URL: https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt
```{r}
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt"
negWords <- scan(URL, character(0), sep = "\n")
negWords <- posWords[-1:-34]
```

#There should be 2006 positive words and 4783 negative words.

#8.	Explain what the following lines of code does and comment each line. Then add similar code for the negative words.
```{r}
posDFM <- dfm_match(tweetDFM, posWords) 
#This code matches the positive words with the tweetDFM dataframe
posFreq <- textstat_frequency(posDFM)
#This code tells the frequency of positive words in the documents
negDFM <- dfm_match(tweetDFM, negWords)
#This code matches the negative words with the tweetDFM dataframe
negFreq <- textstat_frequency(negDFM)
#This code tells the frequency of negative words in the documents.
```

#9.	Explore posFreq and negFreq using str() or glimpse(). Explain the fields in these data frames.
```{r}
library(tidyverse)
str(posFreq)
glimpse(posFreq)
# posFreq contains all the positive words in the feature column with
#the number of frequencies of the particular word and rank them based
# on the frequency of the number.
str(negFreq)
glimpse(negFreq)
# negFreq contains all the negative words in the feature column with
#the number of frequencies of the particular word and rank them based
# on the frequency of the number.
```


#10.	 Output the 10 most frequently occurring positive and negative words including how often each occurred.
```{r}
posFreq[1:10,]
negFreq[1:10,]
```


